{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Hugo Lyons Keenan\n",
    "\n",
    "Aim: This script retrieves data for all years within a user-specified year range and saves it to a file with a custom format, and cleans the resulting file.\n",
    "\n",
    "Note: To run this script successfully, you need to get your API key from the NYT Developer site and save it in a file called \"nyt_api_key.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tqdm if not already installed\n",
    "%pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_corpus import get_corpus, concat_data\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('NYT downloader')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Example of how to use the get_corpus function\n",
    "start_year = 2000\n",
    "end_year = 2001\n",
    "delimiter = '|||||'\n",
    "missing_value = 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corpus(start_year, end_year, logger, delimiter, missing_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'nyt_corpus.csv'\n",
    "concat_data(out_file=out_file, dir_path='corpus', logger=logger)  # should be 1126 files (as we deleted two for containing no data - nyt_data_1978_9.csv; nyt_data_1978_10.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['title', 'section_name', 'snippet', 'lead_paragraph', 'year', 'month', 'web_url']\n",
      "Header has 7 columns\n",
      "Processing chunk 1000\n",
      "Processing chunk 2000\n",
      "Processing chunk 3000\n",
      "Processing chunk 4000\n",
      "Processing chunk 5000\n",
      "Processing chunk 6000\n",
      "Processing chunk 7000\n",
      "Processing chunk 8000\n",
      "Processing chunk 9000\n",
      "Processing chunk 10000\n",
      "Number of malformed rows: 116799\n",
      "Number of removed rows: 79\n"
     ]
    }
   ],
   "source": [
    "# Clean the large data file\n",
    "import re\n",
    "\n",
    "def read_csv_multi_char_delimiter(file_path, delimiter, chunk_size = 1000):\n",
    "    \"\"\"\n",
    "    A generator function to read a large CSV file with a multi-character delimiter and yield chunks of rows.\n",
    "    Also checks for malformed rows and raises an error if the number of columns is not the same as the header.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Escape special regex characters in the delimiter\n",
    "        escaped_delimiter = re.escape(delimiter)\n",
    "        # Read and split the header\n",
    "        header = re.split(escaped_delimiter, f.readline().strip())\n",
    "        print(f\"Header: {header}\")\n",
    "        expected_column_count = len(header)\n",
    "        print(f\"Header has {expected_column_count} columns\")\n",
    "        n_malformed_rows = 0\n",
    "        chunk = []\n",
    "        working_line = ''\n",
    "        for row_num, line in enumerate(f, start=2): \n",
    "            working_line += line.strip('\\n')\n",
    "            row = re.split(escaped_delimiter, working_line)\n",
    "            # If the last element of the row is not a URL, it is malformed and we should look for the \n",
    "            # rest of the row in the next line\n",
    "            if row[-1][0:4] != 'http':\n",
    "                n_malformed_rows += 1\n",
    "                continue\n",
    "            \n",
    "            # If the row has a URL, but the number of columns is not as expected, it is malformed\n",
    "            # and this should be raised as an error as there is likely missing data\n",
    "            if row[-1][0:4] == 'http' and len(row) != expected_column_count:\n",
    "                raise ValueError(f\"Row {row_num} has {len(row)} columns, expected {expected_column_count}\")\n",
    "            \n",
    "            chunk.append(row)\n",
    "            working_line = ''\n",
    "            if len(chunk) == chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        print(f\"Number of malformed rows: {n_malformed_rows}\")\n",
    "        if chunk:  \n",
    "            yield chunk\n",
    "\n",
    "# Usage\n",
    "file_path = 'nyt_corpus.csv'\n",
    "delimiter = '|||||' \n",
    "chunk_size = 1000\n",
    "new_file_path = 'nyt_corpus_cleaned.csv'\n",
    "header = ['title', 'section_name', 'snippet', 'lead_paragraph', 'year', 'month', 'web_url']\n",
    "invalid_rows = []\n",
    "\n",
    "# make sure that the new file is empty before writing to it\n",
    "open(new_file_path, 'w').close()\n",
    "# write the header to the new file\n",
    "with open(new_file_path, 'a') as file:\n",
    "    file.write(delimiter.join(header) + '\\n')\n",
    "for chunk_num, chunk in enumerate(read_csv_multi_char_delimiter(file_path, delimiter, chunk_size), start=1):\n",
    "    if chunk_num % 1000 == 0:\n",
    "        print(f\"Processing chunk {chunk_num}\")\n",
    "    # check if the chunk has the correct number of columns before writing to the new file using the delimiter\n",
    "    to_write = []\n",
    "    for row in chunk:\n",
    "        if len(row) != 7:\n",
    "            raise ValueError(f\"Row has {len(row)} columns, expected 7\")\n",
    "        # make sure that the year is:\n",
    "        # 1. a 4-digit number\n",
    "        # 2. in the range 1930-2023\n",
    "        year = row[header.index('year')]\n",
    "        if (not year.isdigit()) or len(year) != 4 or int(year) < 1930 or int(year) > 2023:\n",
    "            # add row to the list of invalid rows\n",
    "            invalid_rows.append(row)\n",
    "            continue\n",
    "        to_write.append(row)\n",
    "        \n",
    "    string_to_write = '\\n'.join([delimiter.join(row) for row in to_write]) + '\\n'\n",
    "    with open(new_file_path, 'a') as file:\n",
    "        file.write(string_to_write)\n",
    "print(f\"Number of removed rows: {len(invalid_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['title', 'section_name', 'snippet', 'lead_paragraph', 'year', 'month', 'web_url']\n",
      "Header has 7 columns\n",
      "Processing chunk 1000\n",
      "Processing chunk 2000\n",
      "Processing chunk 3000\n",
      "Processing chunk 4000\n",
      "Processing chunk 5000\n",
      "Processing chunk 6000\n",
      "Processing chunk 7000\n",
      "Processing chunk 8000\n",
      "Processing chunk 9000\n",
      "Processing chunk 10000\n",
      "Number of malformed rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the cleaned file -> should result in 0 malformed rows\n",
    "# Also collect frequency of different entries\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(lambda: defaultdict(int))\n",
    "for chunk_num, chunk in enumerate(read_csv_multi_char_delimiter(new_file_path, delimiter, chunk_size), start=1):\n",
    "    if chunk_num % 1000 == 0:\n",
    "        print(f\"Processing chunk {chunk_num}\")\n",
    "    for row in chunk:\n",
    "        if len(row) != 7:\n",
    "            raise ValueError(f\"Row has {len(row)} columns, expected 7\")\n",
    "        if not row[header.index('year')].isdigit() or len(row[header.index('year')]) != 4:\n",
    "            raise ValueError(f\"Row has an invalid year: {row[header.index('year')]}\")\n",
    "        if int(row[header.index('year')]) < 1930 or int(row[header.index('year')]) > 2023:\n",
    "            raise ValueError(f\"Row has an invalid year: {row[header.index('year')]}\")\n",
    "        for column in ['section_name', 'year', 'month']:\n",
    "            counter[column][row[header.index(column)]] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values for section_name: 91 e.g. ['Archives', '|Archives', '||Archives', 'Books', 'Business Day']\n",
      "Number of unique values for year: 94 e.g. ['1930', '1931', '1932', '1933', '1934']\n",
      "Number of unique values for month: 12 e.g. ['01', '02', '03', '04', '05']\n"
     ]
    }
   ],
   "source": [
    "# print the number of unique values for each column\n",
    "for column, values in counter.items():\n",
    "    print(f\"Number of unique values for {column}: {len(values)} e.g. {list(values.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NYT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
