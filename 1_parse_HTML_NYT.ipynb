{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Naomi Baes and Chat GPT //\n",
    "Source: https://osf.io/uya29/ ; Authors: Salvatore Giorgi, Daniel Roy Sadek Habib, Douglas Bellew, Garrick Sherman, and Brenda Curtis//\n",
    "Aim: This script is designed to parse HTML files containing articles from The New York Times website. It extracts various metadata fields such as the article title, ID, publication date, section, and paragraph text from HTML files and organizes them into a structured format (e.g., CSV or TSV). Additionally, it handles certain cases where the HTML files do not contain actual articles, logging these cases as errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import glob\n",
    "import csv\n",
    "import regex\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import requests  # Import requests to download HTML content\n",
    "\n",
    "DEFAULT_DATA_DIRECTORY = \"C:/Users/naomi/OneDrive/COMP80004_PhDResearch/RESEARCH/DATA/CORPORA/MEDIA/NYT/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function checks if a given filename is not present in the \"NoArticle\" list of the error_list dictionary.\n",
    "# If the filename is not present, it appends the filename to the \"ErrorWithArticle\" list in the error_list dictionary.\n",
    "def check_for_no_article(error_list, filename):\n",
    "    \n",
    "    if (filename not in error_list[\"NoArticle\"]):\n",
    "        if \"ErrorWithArticle\" not in error_list:\n",
    "            error_list[\"ErrorWithArticle\"] = []\n",
    "        error_list[\"ErrorWithArticle\"].append(filename)\n",
    "\n",
    "# This function is responsible for parsing an HTML file and extracting specific fields listed in the field_list.\n",
    "# It uses BeautifulSoup to parse the HTML content.\n",
    "# It checks various conditions in the HTML content to determine if the file should be skipped or if it contains errors.\n",
    "# If the HTML file meets certain conditions (e.g., absence of an article tag, presence of specific meta tags), it adds the filename to the corresponding error list in the error_list dictionary.\n",
    "# If the file is deemed valid, it extracts fields such as title, article_id, publish_date, section, keyword_list, and paragraph_text.\n",
    "# It then constructs a DataFrame (df) containing the extracted data.\n",
    "# Finally, it returns the DataFrame.\n",
    "def parse_html_file(filename, field_list, error_list):\n",
    "    #\"title\", \"article_id\", \"paragraph_num\", \"publish_date\", \"section\", \"paragraph_text\", [keyword_list]\n",
    "\n",
    "    #row_list = []\n",
    "    df = pd.DataFrame(columns=field_list)\n",
    "    with open(filename,\"r\") as in_file:\n",
    "        html_tree = BeautifulSoup(in_file, \"lxml\")\n",
    "        row_dict = {}\n",
    "        try:\n",
    "            #Search for skippable files:\n",
    "            #Doesn't contain an article tag\n",
    "            no_article = False\n",
    "            if (len(html_tree.find_all(\"article\")) <=0):\n",
    "                no_article = True\n",
    "                if \"NoArticle\" not in error_list:\n",
    "                    error_list[\"NoArticle\"] = []\n",
    "                error_list[\"NoArticle\"].append(filename.split(\"/\")[-1])\n",
    "                return df\n",
    "            #<meta property=\"article:tag\" content=\"Word of the Day\">\n",
    "            if (len(html_tree.find_all(\"meta\", attrs={\"content\":\"Word of the Day\"})) > 0):\n",
    "                #print(\"May be a Word of the Day - skip file:\" + filename)\n",
    "                if \"WordofDay\" not in error_list:\n",
    "                    error_list[\"WordofDay\"] = []\n",
    "                error_list[\"WordofDay\"].append(filename.split(\"/\")[-1])\n",
    "                #check_for_no_article(error_list, filename.split(\"/\")[-1])\n",
    "                return df\n",
    "            #<nav class=\"css-k38v3p\" aria-labelledby=\"showcontrols\">\n",
    "            elif (len(html_tree.find_all(\"nav\", attrs={\"aria-labelledby\":\"showcontrols\"})) > 0):\n",
    "                #print(\"May be a slideshow - skip file:\"+filename)\n",
    "                if \"Slideshow\" not in error_list:\n",
    "                    error_list[\"Slideshow\"] = []\n",
    "                error_list[\"Slideshow\"].append(filename.split(\"/\")[-1])\n",
    "                #check_for_no_article(error_list, filename.split(\"/\")[-1])\n",
    "                return df\n",
    "            #<meta name=\"applicationName\" content=\"interactive\" id=\"applicationName\">\n",
    "            elif (len(html_tree.find_all(\"meta\", attrs={\"content\":\"interactive\"})) > 0):\n",
    "                #print(\"May be a interactive poll - skip file:\"+filename)\n",
    "                if \"InteractivePoll\" not in error_list:\n",
    "                    error_list[\"InteractivePoll\"] = []\n",
    "                error_list[\"InteractivePoll\"].append(filename.split(\"/\")[-1])   \n",
    "                #check_for_no_article(error_list, filename.split(\"/\")[-1])                       \n",
    "                return df \n",
    "            #<div class=\"nytint-discussion-content\">\n",
    "            elif (len(html_tree.find_all(\"div\", attrs={\"class\":\"nytint-discussion-content\"})) > 0):\n",
    "                #print(\"May be a discussion article quoting multiple sources - skip file:\"+filename)\n",
    "                if \"Discussion\" not in error_list:\n",
    "                    error_list[\"Discussion\"] = []\n",
    "                error_list[\"Discussion\"].append(filename.split(\"/\")[-1])  \n",
    "                #check_for_no_article(error_list, filename.split(\"/\")[-1])                         \n",
    "                return df             \n",
    "            #<meta property=\"article:tag\" content=\"Picture Prompt\">\n",
    "            elif (len(html_tree.find_all(\"meta\", attrs={\"property\":\"article:tag\",\"content\":\"Picture Prompt\"})) > 0):\n",
    "                #print(\"May be a picture prompt - skip file:\"+filename)\n",
    "                if \"PicturePrompt\" not in error_list:\n",
    "                    error_list[\"PicturePrompt\"] = []\n",
    "                error_list[\"PicturePrompt\"].append(filename.split(\"/\")[-1])  \n",
    "                #check_for_no_article(error_list, filename.split(\"/\")[-1])                          \n",
    "                return df\n",
    "            # These items and others I eventually found all had the \"No Article Tag\" Commonality\n",
    "            #<meta property=\"article:tag\" content=\"News Q's\">         \n",
    "            #elif (len(html_tree.find_all(\"meta\", attrs={\"property\":\"article:tag\",\"content\":\"News Q's\"})) > 0):\n",
    "            #    print(\"May be a question prompt - skip file:\"+filename)            \n",
    "            #    return df  \n",
    "            #<meta property=\"article:tag\" content=\"Student Opinion\">\n",
    "            #elif (len(html_tree.find_all(\"meta\", attrs={\"property\":\"article:tag\",\"content\":\"Student Opinion\"})) > 0):\n",
    "            #    print(\"May be a student response prompt - skip file:\"+filename)            \n",
    "            #    return df \n",
    "            \n",
    "            # These items are snippet refereces to older articles\n",
    "            #<meta property=\"article:tag\" content=\"In Our Pages\">\n",
    "            #<meta name=\"SCG\" content=\"iht-retrospective\">\n",
    "            elif ((len(html_tree.find_all(\"meta\", attrs={\"property\":\"article:tag\",\"content\":\"In Our Pages\"})) > 0) or\n",
    "                  (len(html_tree.find_all(\"meta\", attrs={\"name\":\"SCG\",\"content\":\"iht-retrospective\"})) > 0)) :\n",
    "                #print(\"May be a link to a historical article - skip file:\"+filename)        \n",
    "                if \"HistoricalArticle\" not in error_list:\n",
    "                    error_list[\"HistoricalArticle\"] = []\n",
    "                error_list[\"HistoricalArticle\"].append(filename.split(\"/\")[-1])\n",
    "                #check_for_no_article(error_list, filename.split(\"/\")[-1])\n",
    "                return df\n",
    "            #Articles that were just either an audio or video file\n",
    "            #<meta data-rh=\"true\" property=\"og:url\" content=\"https://www.nytimes.com/audio/2016/09/13/insider/13-Insider-Pete-Audio.html\">\n",
    "            #<meta data-rh=\"true\" property=\"og:url\" content=\"https://www.nytimes.com/video/world/middleeast/100000004643525/postcards-from-the-hajj-the-crowds.html\">\n",
    "            elif ((len(html_tree.find_all(\"meta\", attrs={\"property\":\"og:url\"},content=regex.compile(\"/audio/\"))) > 0) or\n",
    "                  (len(html_tree.find_all(\"meta\", attrs={\"property\":\"og:url\"},content=regex.compile(\"/video/\"))) > 0)) :\n",
    "                #print(\"May be a link to a audio or video - skip file:\"+filename)            \n",
    "                if \"AVFile\" not in error_list:\n",
    "                    error_list[\"AVFile\"] = []\n",
    "                error_list[\"AVFile\"].append(filename.split(\"/\")[-1])\n",
    "                #check_for_no_article(error_list, filename.split(\"/\")[-1])\n",
    "                return df\n",
    "            \n",
    "            #Double check if any files had no artile tag but weren't caught by the above tags\n",
    "            # Superseeded by making having \"No Article\" an error instead of a warning\n",
    "            if (no_article):\n",
    "                if \"NoErrorWithNoArticle\" not in error_list:\n",
    "                    error_list[\"NoErrorWithNoArticle\"] = []\n",
    "                error_list[\"NoErrorWithNoArticle\"].append(filename)\n",
    "            \n",
    "            #\n",
    "            # Should be Valid File - Parse HTML by Tag listed in the field list\n",
    "            # Ordering shouldn't matter as we're putting the data into a dataframe (with ordered columns)\n",
    "            # before writing out the data\n",
    "            #\n",
    "            if (\"title\" in field_list):\n",
    "                #<meta data-rh=\"true\" property=\"og:title\" content=\"Opinion | Trump Picks Wall Street Over Main Street (Published 2017)\">\n",
    "                #<meta property=\"og:title\" content=\"Introducing kyt — Our Web App Configuration Toolkit\">\n",
    "                row_dict[\"title\"] = html_tree.find(\"meta\", property=\"og:title\", content=True)\n",
    "                if row_dict[\"title\"] is None:\n",
    "                    print(\"No title found in file:\"+filename)\n",
    "                else:\n",
    "                    row_dict[\"title\"] = row_dict[\"title\"][\"content\"]\n",
    "                    if (row_dict[\"title\"].find(\"(Published\") >= 0):\n",
    "                        row_dict[\"title\"] = row_dict[\"title\"][0:row_dict[\"title\"].find(\"(Published\")].rstrip()     \n",
    "                #print(row_dict[\"title\"])\n",
    "            \n",
    "            if (\"article_id\" in field_list):\n",
    "                #<meta data-rh=\"true\" name=\"articleid\" content=\"100000004912753\">\n",
    "                #<meta itemprop=\"identifier\" name=\"blogpostid\" content=\"100000004644202\">\n",
    "                #<article class=\"post-7322 post type-post status-publish hentry category-code category-open-source tag-configuration tag-open-source tag-toolkit des-computers-and-the-internet des-open-source-software org-new-york-times\" id=\"post-7322\">\n",
    "                row_dict[\"article_id\"] = html_tree.find(\"meta\",  attrs={\"name\":\"articleid\"}, content=True)\n",
    "                if row_dict[\"article_id\"] is None:\n",
    "                    row_dict[\"article_id\"] = html_tree.find(\"meta\", attrs={\"itemprop\":\"identifier\"}, content=True)\n",
    "                    if row_dict[\"article_id\"] is None:\n",
    "                        if (len(html_tree.find_all(\"article\", id=True)) > 0):\n",
    "                            row_dict[\"article_id\"] = html_tree.find_all(\"article\")[0][\"id\"]\n",
    "                        else:             \n",
    "                            print(\"No article_id found in file:\"+filename)\n",
    "                    else:\n",
    "                        row_dict[\"article_id\"] = row_dict[\"article_id\"][\"content\"]\n",
    "                else:\n",
    "                    row_dict[\"article_id\"] = row_dict[\"article_id\"][\"content\"]\n",
    "                #print(row_dict[\"article_id\"])\n",
    "            \n",
    "            if (\"publish_date\" in field_list):\n",
    "                #<meta data-rh=\"true\" property=\"article:published_time\" content=\"2017-02-04T17:30:57.000Z\">\n",
    "                row_dict[\"publish_date\"] = html_tree.find(\"meta\",property=\"article:published_time\", content=True)\n",
    "                if row_dict[\"publish_date\"] is None:\n",
    "                    print(\"No publish_date found in file:\"+filename)\n",
    "                else:\n",
    "                    row_dict[\"publish_date\"] = row_dict[\"publish_date\"][\"content\"]\n",
    "                    # Possilbe responses: 1474096173, 2016-09-17T05:00:29.000Z\n",
    "                    if (len(row_dict[\"publish_date\"]) == 10):\n",
    "                        try:\n",
    "                            date_num = int(row_dict[\"publish_date\"])\n",
    "                            date_time = datetime.datetime.fromtimestamp(date_num)\n",
    "                            row_dict[\"publish_date\"] = date_time.isoformat(timespec=\"milliseconds\") + \"Z\"\n",
    "                            #print(\"Converting datestamp in file:\"+filename)\n",
    "                        except ValueError:\n",
    "                            # Not a number - just leave the date as whatever came in\n",
    "                            pass\n",
    "                #print(row_dict[\"publish_date\"])\n",
    "            \n",
    "            if (\"section\" in field_list):\n",
    "                #<meta data-rh=\"true\" property=\"article:section\" content=\"Opinion\">\n",
    "                row_dict[\"section\"] = html_tree.find(\"meta\", property=\"article:section\", content=True)\n",
    "                if row_dict[\"section\"] is None:\n",
    "                    print(\"No section found in file:\"+filename)\n",
    "                else:\n",
    "                    row_dict[\"section\"] = row_dict[\"section\"][\"content\"]\n",
    "                    # We found a section tag, but it was blank in the data.  Replace with someting.\n",
    "                    if (row_dict[\"section\"] == \"\"):\n",
    "                        row_dict[\"section\"] = \"No Section\"\n",
    "                #print(row_dict[\"section\"])\n",
    "            \n",
    "            if (\"keyword_list\" in field_list):\n",
    "                #<meta data-rh=\"true\" name=\"news_keywords\" content=\"Banking and Finance,Donald Trump,Executive Orders,Dodd Frank,Regulation and Deregulation\">\n",
    "                #<meta name=\"keywords\" content=\"Computers and the Internet,Open-Source Software,New York Times,Code,Open Source\">\n",
    "                row_dict[\"keyword_list\"] = html_tree.find(\"meta\", attrs={\"name\":\"news_keywords\"}, content=True)\n",
    "                if row_dict[\"keyword_list\"] is None:\n",
    "                    row_dict[\"keyword_list\"] = html_tree.find(\"meta\", attrs={\"name\":\"keywords\"}, content=True)\n",
    "                    if row_dict[\"keyword_list\"] is None:\n",
    "                        print(\"No keyword_list found in file:\"+filename)\n",
    "                    else:            \n",
    "                        row_dict[\"keyword_list\"] = row_dict[\"keyword_list\"][\"content\"]\n",
    "                else:\n",
    "                    row_dict[\"keyword_list\"] = row_dict[\"keyword_list\"][\"content\"]\n",
    "                #print(row_dict[\"keyword_list\"])\n",
    "        \n",
    "            if ((\"paragraph_text\" in field_list) or\n",
    "                (\"paragraph_num\" in field_list)):\n",
    "                #<p class=\"css-axufdj evys1bk0\">President Trump fired the first round in his war against financial regulations by signing two executive orders on Friday.</p>\n",
    "                #<p class=\"story-body-text\" itemprop=\"articleBody\"><img src=\"https://static01.nyt.com/images/blogs/open/2016/ios-fire-emoji.png\" alt=\"Fire emoji\" width=\"20px\"><strong>Welcome to configuration hell</strong></p>\n",
    "                #<div class=\"listy_body\">\n",
    "                    #<p><span>On the first day of summer in 2005, Snapple sought to break a Guinness World Record by erecting a 25-foot, 35,000-pound tower of flavored ice in Union Square. The kiwi-strawberry pillar was to be the world’s largest Popsicle. If only the sun had cooperated.</span> </p>\n",
    "                #<section name=\"articleBody\" class=\"meteredContent css-1r7ky0e\">\n",
    "                paragraphs = html_tree.find_all(\"p\", class_=\"css-axufdj evys1bk0\")\n",
    "                if (len(paragraphs) <= 0):\n",
    "                    paragraphs = html_tree.find_all(\"p\", class_=\"story-body-text\", itemprop=\"articleBody\")\n",
    "                    if (len(paragraphs) <= 0):\n",
    "                        possible_paragraphs = html_tree.find_all(\"div\", class_=\"listy_body\")\n",
    "                        if (len(possible_paragraphs) > 0):\n",
    "                            for pp in possible_paragraphs:\n",
    "                                paragraph = pp.find(\"p\")\n",
    "                                if paragraph is not None:\n",
    "                                    paragraphs.append(paragraph)\n",
    "                        if (len(paragraphs) <= 0):\n",
    "                            possible_sections = html_tree.find_all(\"section\", attrs={\"name\":\"articleBody\"})\n",
    "                            if (len(possible_sections) > 0):\n",
    "                                for ps in possible_sections:\n",
    "                                    possible_paragraphs = ps.find_all(\"p\")\n",
    "                                    for pp in possible_paragraphs:\n",
    "                                        paragraphs.append(pp)\n",
    "                                if (len(paragraphs) <= 0):\n",
    "                                    print(\"No paragraphs found for file:\"+filename)\n",
    "                            \n",
    "                for i, paragraph in enumerate(paragraphs):\n",
    "                    if (\"paragraph_num\" in field_list):\n",
    "                        row_dict[\"paragraph_num\"] = i\n",
    "                    if (\"paragraph_text\" in field_list):\n",
    "                        # Take out newlines, tabs and multiple spaces from wordwraps in the html file\n",
    "                        row_dict[\"paragraph_text\"] = paragraph.text.strip().replace(\"\\n\", \"\").replace(\"\\t\",\"\")\n",
    "                        row_dict[\"paragraph_text\"] = regex.sub(\"[ ]+\", \" \", row_dict[\"paragraph_text\"])\n",
    "                        #print(row_dict[\"paragraph_text\"])\n",
    "                    # Don't include empty paragraphs in the list of paragraphs to put into the text file\n",
    "                    if (not (row_dict[\"paragraph_text\"] == \"\")):\n",
    "                        df = df.append(row_dict,ignore_index=True)\n",
    "            #print(df[\"paragraph_text\"])\n",
    "        except TypeError as err:\n",
    "            print(\"Following issue with file:\"+filename)\n",
    "            print(err)                    \n",
    "    return df\n",
    "\n",
    "# This function outputs the results (DataFrame) to a specified file.\n",
    "# It prints the filename and the number of paragraphs in the DataFrame.\n",
    "# It writes the DataFrame to the specified file using the specified delimiter.\n",
    "def output_result_file(file, df, delimiter):\n",
    "    print(file)\n",
    "    print(\"Paragraphs :\" +str(len(df)))\n",
    "    df.to_csv(file, index = False, sep=delimiter, header=False)\n",
    "\n",
    "# This function outputs the error statistics to a specified file.\n",
    "# It prints the filename and the error statistics, including the number of occurrences for each error type.\n",
    "# It writes the error_list dictionary to the specified file in JSON format.\n",
    "def output_error_file(file, error_list):\n",
    "    print(file)\n",
    "    print(\"Error Statistics:\")\n",
    "    for key in error_list.keys():\n",
    "        print(\"\\t\"+key+ \" has been encountered: \"+str(len(error_list[key]))+ \" times.\" )\n",
    "    with open(file,\"w\") as out_file:\n",
    "        json.dump(error_list, out_file, indent=\"\\t\")\n",
    "\n",
    "# This is the main function that orchestrates the parsing process.\n",
    "# It defines command-line arguments using the argparse module to specify input and output directories/files.\n",
    "# It initializes variables such as field_list and error_list.\n",
    "# It collects a list of HTML files to parse from the specified directory.\n",
    "# It iterates through each HTML file, parsing it using the parse_html_file function and accumulating the results in a DataFrame.\n",
    "# It outputs the results and error statistics to specified files.\n",
    "# It provides feedback on the progress and completion of the parsing process.       \n",
    "def main():\n",
    "    print(\"parse_NYT_HTML.py started at :\" + datetime.datetime.now().isoformat())\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument(\"--data_dir\", action=\"store\", dest=\"data_dir\", required=False,\n",
    "                        default=DEFAULT_DATA_DIRECTORY,\n",
    "                        help=\"Full Path to location of HTML files to parse\")\n",
    "    parser.add_argument(\"--data_files\", action=\"store\", dest=\"data_files\", required=False,\n",
    "                        default=\"*.html\",\n",
    "                        #default=\"0050*.html\",\n",
    "                        #default=\"0050001.html\", \n",
    "                        #default=\"0055001.html\",\n",
    "                        #default=\"0060001.html\",\n",
    "                        #default=\"0065001.html\",\n",
    "                        #default=\"0070001.html\",\n",
    "                        #default=\"0075389.html\", \n",
    "                        help=\"Names of HTML files to parse\")\n",
    "    parser.add_argument(\"--results_dir\", action=\"store\", dest=\"results_dir\", required=False,\n",
    "                        default=\"/home/douglasvbellew/Workspaces/NYT_HTML_Parse/results_second\", \n",
    "                        help=\"Full Path to location to store results\")\n",
    "    parser.add_argument(\"--result_filename\", action='store', dest='result_filename', required=False,\n",
    "                        default=\"nyt_html_data_2.tsv\", help='Name of results file')\n",
    "    parser.add_argument(\"--error_filename\", action='store', dest='error_filename', required=False,\n",
    "                        default=\"nyt_html_error_2.json\", help='Name of error file')\n",
    "    parser.add_argument(\"--field_delimieter\", action='store', dest='field_delimiter', required=False,\n",
    "                        default=\"\\t\", help=\"inter-field separator (generally \\\"\\\\t\\\" or \\\",\\\")\")\n",
    "\n",
    "    in_args = parser.parse_args()\n",
    "    output_filename = os.path.join(in_args.results_dir,in_args.result_filename)\n",
    "    error_filename = os.path.join(in_args.results_dir,in_args.error_filename)\n",
    "    data_files = glob.glob(os.path.join(in_args.data_dir,in_args.data_files))\n",
    "    \n",
    "    #field_list = [\"title\", \"article_id\", \"paragraph_num\", \"publish_date\", \"section\", \"paragraph_text\", \"keyword_list\"]\n",
    "    field_list = [\"title\", \"article_id\", \"paragraph_num\", \"publish_date\", \"section\", \"paragraph_text\"]\n",
    "    error_list = {}\n",
    "    if (len(data_files) > 0):\n",
    "        result_df = pd.DataFrame(columns=field_list)\n",
    "        for num, filename in enumerate(data_files):\n",
    "            if (num%500 == 0):\n",
    "                print(datetime.datetime.now().isoformat() + \" \" + filename)\n",
    "            html_df = parse_html_file(filename, field_list, error_list)\n",
    "            #print(html_df)\n",
    "            #print(error_list)\n",
    "            result_df = pd.concat([result_df,html_df], ignore_index=True)\n",
    "        output_result_file(output_filename, result_df, in_args.field_delimiter)\n",
    "        output_error_file(error_filename,error_list)\n",
    "        print(\"parse_NYT_HTML.py finsihed - status nominal\")\n",
    "    else:\n",
    "        print(\"Filename: \"+in_args.data_files+\" not found in directory:\"+in_args.data_dir)\n",
    "        print(\"parse_NYT_HTML.py finsihed - status failure\")\n",
    "    print(\"Finished at: \" + datetime.datetime.now().isoformat())    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
